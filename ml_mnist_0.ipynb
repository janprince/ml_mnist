{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447d05b3",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this notebook, we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied\n",
    "so much that it is often called the “Hello World” of Machine Learning: whenever people come up with a new classification algorithm, they are curious to see how it will perform on MNIST. Whenever someone learns Machine Learning, sooner or\n",
    "later they tackle MNIST. \n",
    "\n",
    "Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
    "one of them. The following code fetches the MNIST dataset:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7cb943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from sklearn.datasets import load_digits\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = load_digits()\n",
    "\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15c04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# main data\n",
    "X = mnist[\"data\"]\n",
    "\n",
    "# target array\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c31f175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADd0lEQVR4nO3dsW0iURRA0WXlHKgAOjFUYJdCCW7JHQyVgEuggtkGLDlZ/7lizgkheD+5etIkbzPP8x+g5+/SDwC+J06IEidEiROixAlRLz/8/5SfcqdpGjrvfD4Pm7XdbofN+vj4GDbrcrkMm7WAzXc/2pwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I+ukcw1O63+9LP+HXPB6PYbN2u92wWWtkc0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFqlecY9vv90k94CsfjceknPDWbE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKhV3kq53W5LP+HXbLfbYbNOp9OwWWtkc0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFqlecYpmla+gm/ZrfbLf0E/hObE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVGrPMfw/v4+dN7n5+fQeTwHmxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRqzzH8My+vr6GzRp5ZuLt7W3YrAqbE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKhV3ko5nU5D572+vg6bdb1eh806HA7DZq2RzQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSozTzPS78B+IbNCVHihChxQpQ4IUqcECVOiPoH6LonymqpJUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# a sample instance\n",
    "random_num = random.randint(0, 1100)\n",
    "some_digit = X[random_num]\n",
    "some_digit_image = some_digit.reshape(8, 8)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(y[random_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4450c45",
   "metadata": {},
   "source": [
    "#### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2bf97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[: 1100], X[1100:], y[: 1100], y[1100:]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(1100)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995786fb",
   "metadata": {},
   "source": [
    "### Training a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caffc9",
   "metadata": {},
   "source": [
    "Simplifying the problem for now and only try to identify one digit—for example, the number 5. This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for this classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496a406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_5 = (y_train == 5)     # True for all 5s; False for all other digits\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# Now our y arrays have only two classes; True and False (5 or not 5)\n",
    "\n",
    "y_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a02e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\"\"\"\n",
    "The SGDClassifier relies on randomness during training (hence the name “stochastic”). If you want reproducible results, you\n",
    "should set the random_state parameter.\n",
    "\"\"\"\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# predict on an instance\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af279863",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2fce1",
   "metadata": {},
   "source": [
    "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance measures available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04974ef3",
   "metadata": {},
   "source": [
    "#### 1. Measuring Accuracy Using Cross Validation\n",
    "A good way to evaluate a model is to use cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591abbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98637602, 0.99182561, 0.99180328])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a7a9b",
   "metadata": {},
   "source": [
    "Wow! Above 96% accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn’t it? \n",
    "Well, before you get too excited, let’s look at a very dumb classifier that just classifies every single image in the “not-5” class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a80356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92098093, 0.88555858, 0.88797814])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()\n",
    "\n",
    "# evaluating dumb classfier with cross-validation\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d7bb1",
   "metadata": {},
   "source": [
    "That’s right, it has over 88% accuracy! This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time. Beats Nostradamus.\n",
    "\n",
    "This demonstrates why accuracy is generally **not the preferred performance measure for classifiers**, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).\n",
    "\n",
    "Also known as Class Imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c807464",
   "metadata": {},
   "source": [
    "#### 2. Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the confusion matrix. \n",
    "\n",
    "The general idea is to count the number of times instances of class A are classified as class B. \n",
    "For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n",
    "\n",
    "To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let’s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch. Instead, you can use the cross_val_predict() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f192d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb5970b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[984,   4],\n",
       "       [  7, 105]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
